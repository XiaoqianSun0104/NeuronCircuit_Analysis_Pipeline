{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22b9789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqiansun/opt/anaconda3/lib/python3.8/site-packages/oasis/functions.py:13: UserWarning: Could not find cvxpy. Don't worry, you can still use OASIS, just not the slower interior point methods we compared to in the papers.\n",
      "  warn(\"Could not find cvxpy. Don't worry, you can still use OASIS, \" +\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from oasis.plotting import simpleaxis\n",
    "from oasis.oasis_methods import oasisAR1, oasisAR2\n",
    "from oasis.functions import gen_data, gen_sinusoidal_data, deconvolve, estimate_parameters\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398460d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep info\n",
    "# contact: c\n",
    "# status inlcude: r, s, a\n",
    "\n",
    "root_path = r'/Users/xiaoqiansun/Desktop/MedLu/TubeTest/Data'\n",
    "\n",
    "\n",
    "days = 6\n",
    "fpsN = 15\n",
    "fpsB = 30\n",
    "baselinePeriod = 180 #(3min(180s) baseline)\n",
    "redundentTime = 190 #10s+3min\n",
    "\n",
    "\n",
    "M32_list = ['32-1_28', '32-2_18', '32-3_15', '32-4_20', '32-5_12', '32-6_15']\n",
    "M33_list = ['33-1_28', '33-2_35', '33-3_32', '33-4_30', '33-5_30', '33-6_26']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48789b96",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d18bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep certain status frrame interval\n",
    "#----------------------------------------------------------------------------------\n",
    "def pick_BInterval(m1_tube, m2_tube, s1, s2, c=None):      \n",
    "      \n",
    "    df1 = m1_tube.copy()\n",
    "    df2 = m2_tube.copy()\n",
    "    \n",
    "    if c:\n",
    "        df1 = df1[(df1['mouse1']== s1) & (df1['mouse2']== s2) & (df1['contact']== c)]\n",
    "        df2 = df2[(df2['mouse1']== s1) & (df2['mouse2']== s2) & (df2['contact']== c)]\n",
    "    \n",
    "    else:\n",
    "        df1 = df1[(df1['mouse1']== s1) & (df1['mouse2']== s2)]\n",
    "        df2 = df2[(df2['mouse1']== s1) & (df2['mouse2']== s2)]\n",
    "    \n",
    "        \n",
    "    df1 = df1.drop(['mouse1', 'mouse2', 'contact'], axis=1)\n",
    "    df2 = df2.drop(['mouse1', 'mouse2', 'contact'], axis=1)\n",
    "    \n",
    "    return(df1, df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate event rate&amplitude using oasis\n",
    "#----------------------------------------------------------------------------------\n",
    "def ER_AM_Oasis(df, fps):\n",
    "        \n",
    "    Frames, n_neuron = df.shape\n",
    "    \n",
    "    if Frames != 0 and n_neuron != 0:\n",
    "        # get array\n",
    "        df_array = df.to_numpy()\n",
    "\n",
    "        spike_num = 0\n",
    "        spike_mean = []\n",
    "        spikeDFF_mean = []\n",
    "\n",
    "        for i in range(n_neuron):\n",
    "            y = df_array[:,i][~np.isnan(df_array[:,i])]\n",
    "            c, s, b, g, lam = deconvolve(y, penalty=1)\n",
    "\n",
    "            # pick non-zero spikes from deconvoluated trace\n",
    "            sI = [i for i, e in enumerate(s) if e != 0]\n",
    "            ss = [e for i, e in enumerate(s) if e != 0]\n",
    "\n",
    "            # eliminate small spikes (only keep ones larger than non-zero mean)\n",
    "            meanS = np.mean(ss)\n",
    "            spikeIndex = [i for i, e in enumerate(ss) if e >= meanS]\n",
    "            spikes = [e for i, e in enumerate(ss) if e >= meanS]\n",
    "\n",
    "            spike_num = spike_num + len(spikeIndex)\n",
    "            spike_mean.append(np.mean(spikes))\n",
    "            spikeDFF_mean.append(np.mean(df_array[:,i][spikeIndex]))\n",
    "        \n",
    "        ER = spike_num/(Frames/fps)/n_neuron\n",
    "        AM_spike = np.nanmean(spike_mean)\n",
    "        AM_DFF = np.nanmean(spikeDFF_mean)\n",
    "    else:\n",
    "        ER = np.nan\n",
    "        AM_spike = np.nan\n",
    "        AM_DFF = np.nan\n",
    "        \n",
    "        \n",
    "    return(ER, AM_spike, AM_DFF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce5596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3f8e88",
   "metadata": {},
   "source": [
    "# Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "254678ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ER_32 = pd.DataFrame({'Days':np.arange(1,7), 'as':np.nan, 'sa': np.nan, 'rs':np.nan, 'sr': np.nan,\n",
    "                      'ar':np.nan, 'ra': np.nan,'ss':np.nan, 'ssc': np.nan,})\n",
    "ER_33 = pd.DataFrame({'Days':np.arange(1,7), 'as':np.nan, 'sa': np.nan, 'rs':np.nan, 'sr': np.nan,\n",
    "                      'ar':np.nan, 'ra': np.nan,'ss':np.nan, 'ssc': np.nan,})\n",
    "AM_32 = pd.DataFrame({'Days':np.arange(1,7), 'as':np.nan, 'sa': np.nan, 'rs':np.nan, 'sr': np.nan,\n",
    "                      'ar':np.nan, 'ra': np.nan,'ss':np.nan, 'ssc': np.nan,})\n",
    "AM_33 = pd.DataFrame({'Days':np.arange(1,7), 'as':np.nan, 'sa': np.nan, 'rs':np.nan, 'sr': np.nan,\n",
    "                      'ar':np.nan, 'ra': np.nan,'ss':np.nan, 'ssc': np.nan,})\n",
    "AM00_32 = pd.DataFrame({'Days':np.arange(1,7), 'as':np.nan, 'sa': np.nan, 'rs':np.nan, 'sr': np.nan,\n",
    "                        'ar':np.nan, 'ra': np.nan,'ss':np.nan, 'ssc': np.nan,})\n",
    "AM00_33 = pd.DataFrame({'Days':np.arange(1,7), 'as':np.nan, 'sa': np.nan, 'rs':np.nan, 'sr': np.nan,\n",
    "                        'ar':np.nan, 'ra': np.nan,'ss':np.nan, 'ssc': np.nan,})\n",
    "\n",
    "Summary_s = [ER_32, AM_32, AM00_32, ER_33, AM_33, AM00_33]\n",
    "Sheets_s = ['ER_32', 'AM_Spike_32', 'AM_DFF_32', 'ER_33', 'AM_Spike_33', 'AM_DFF_33']\n",
    "\n",
    "for day in range(days):\n",
    "    \n",
    "    # read in dataframe\n",
    "    m32_folder = os.path.join(root_path, M32_list[day])\n",
    "    m33_folder = os.path.join(root_path, M33_list[day])\n",
    "    \n",
    "    \n",
    "    m32_tube = pd.read_csv(os.path.join(m32_folder,'m32_tube.csv'), index_col = 'Frame')\n",
    "    m33_tube = pd.read_csv(os.path.join(m33_folder,'m33_tube.csv'), index_col = 'Frame')     \n",
    "    \n",
    "    \n",
    "    # separate sessions\n",
    "    as1, as2 = pick_BInterval(m32_tube, m33_tube, 'a', 's', c=None)\n",
    "    sa1, sa2 = pick_BInterval(m32_tube, m33_tube, 's', 'a', c=None)\n",
    "    rs1, rs2 = pick_BInterval(m32_tube, m33_tube, 'r', 's', c=None)\n",
    "    sr1, sr2 = pick_BInterval(m32_tube, m33_tube, 's', 'r', c=None)\n",
    "    ar1, ar2 = pick_BInterval(m32_tube, m33_tube, 'a', 'r', c=None)\n",
    "    ra1, ra2 = pick_BInterval(m32_tube, m33_tube, 'r', 'a', c=None)\n",
    "    \n",
    "    ss1, ss2 = pick_BInterval(m32_tube, m33_tube, 's', 's', c='nc')\n",
    "    ssc1, ssc2 = pick_BInterval(m32_tube, m33_tube, 's', 's', c='c')\n",
    "    \n",
    "    # calculate ER, AM, AM00\n",
    "    M32_dfs = [as1,sa1,rs1,sr1,ar1,ra1,ss1,ssc1]\n",
    "    M33_dfs = [as2,sa2,rs2,sr2,ar2,ra2,ss2,ssc2]\n",
    "    \n",
    "    er_32s = []; am_32s = []; am00_32s = []\n",
    "    er_33s = []; am_33s = []; am00_33s = []\n",
    "    ss = [er_32s,am_32s,am00_32s,er_33s,am_33s,am00_33s]\n",
    "    for i in range(len(M32_dfs)):\n",
    "        \n",
    "        er32, amS32, amF32 = ER_AM_Oasis(M32_dfs[i], fpsN)\n",
    "        er33, amS33, amF33 = ER_AM_Oasis(M33_dfs[i], fpsN)\n",
    "        er_32s.append(er32); am_32s.append(amS32); am00_32s.append(amF32)\n",
    "        er_33s.append(er33); am_33s.append(amS33); am00_33s.append(amF33)\n",
    "        \n",
    "    \n",
    "    # update into dataframe\n",
    "    for m in range(len(Summary_s)):\n",
    "        Summary_s[m].loc[day, ER_32.columns[1:]] = ss[m]\n",
    "    \n",
    "    \n",
    "    \n",
    "writer = pd.ExcelWriter('Summary_deconvoluatedSpikes.xlsx')\n",
    "for k in range(len(Summary_s)):\n",
    "    Summary_s[k].to_excel(writer, sheet_name=Sheets_s[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058ca46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e5d6ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ERB_32 = pd.DataFrame({'Days':np.arange(1,7), 'ER':np.nan, 'AM_Spike': np.nan, 'AM_DFF':np.nan})\n",
    "ERB_33 = pd.DataFrame({'Days':np.arange(1,7), 'ER':np.nan, 'AM_Spike': np.nan, 'AM_DFF':np.nan})\n",
    "\n",
    "erB_32s = []; amB_32s = []; amDFFB_32s = []\n",
    "erB_33s = []; amB_33s = []; amDFFB_33s = []\n",
    "\n",
    "for day in range(days):\n",
    "    \n",
    "    m32_folder = os.path.join(root_path, M32_list[day])\n",
    "    m33_folder = os.path.join(root_path, M33_list[day])\n",
    "    \n",
    "    \n",
    "    m32_base = pd.read_csv(os.path.join(m32_folder,'m32_base.csv'), index_col = 'Frame')\n",
    "    m33_base = pd.read_csv(os.path.join(m33_folder,'m33_base.csv'), index_col = 'Frame')\n",
    "        \n",
    "    er32, amS32, amF32 = ER_AM_Oasis(m32_base, fpsN)\n",
    "    er33, amS33, amF33 = ER_AM_Oasis(m33_base, fpsN)\n",
    "    \n",
    "    erB_32s.append(er32); amB_32s.append(amS32); amDFFB_32s.append(amF32)\n",
    "    erB_33s.append(er33); amB_33s.append(amS33); amDFFB_33s.append(amF33)\n",
    "        \n",
    "    \n",
    "    for m in range(len(Summary_s)):\n",
    "        Summary_s[m].loc[day, ER_32.columns[1:]] = ss[m]\n",
    "    \n",
    "    \n",
    "ERB_32['ER'] = erB_32s; ERB_32['AM_Spike'] = amB_32s; ERB_32['AM_DFF'] = amDFFB_32s\n",
    "ERB_33['ER'] = erB_33s; ERB_33['AM_Spike'] = amB_33s; ERB_33['AM_DFF'] = amDFFB_33s\n",
    "\n",
    "ERB_32.to_excel(writer, sheet_name='Mouse 32 Baseline')\n",
    "ERB_33.to_excel(writer, sheet_name='Mouse 33 Baseline')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46bb50",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b65c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sheets_32 = ['ER_32', 'AM_Spike_32', 'AM_DFF_32']\n",
    "Sheets_33 = ['ER_33', 'AM_Spike_33', 'AM_DFF_33']\n",
    "\n",
    "\n",
    "def plot_ER_DF(sheetIndex, bevavior, whichIndicator, compareOrNot=False):\n",
    "    \n",
    "    SummaryD1 = pd.read_excel('Summary_deconvoluatedSpikes.xlsx', sheet_name=Sheets_32[sheetIndex])\n",
    "    SummaryD2 = pd.read_excel('Summary_deconvoluatedSpikes.xlsx', sheet_name=Sheets_33[sheetIndex])\n",
    "\n",
    "    f, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "    ax.plot(SummaryD1[bevavior], '-.', marker='s', color='orange', label='Mouse 32 '+bevavior+' Deconvoluted')\n",
    "    ax.plot(SummaryD2[bevavior], '-', marker='o', color='orange', label='Mouse 33 '+bevavior+' Deconvoluted')\n",
    "    title = whichIndicator+'At' + bevavior + '_Deconvoluted'\n",
    "    \n",
    "    if compareOrNot:\n",
    "        SummaryF1 = pd.read_excel('Summary.xlsx', sheet_name=Sheets_32[sheetIndex])\n",
    "        SummaryF2 = pd.read_excel('Summary.xlsx', sheet_name=Sheets_33[sheetIndex])\n",
    "        ax.plot(SummaryF1[bevavior], '-.', marker='s', color='blue', label='Mouse 32 '+bevavior+' DFF')\n",
    "        ax.plot(SummaryF2[bevavior], '-', marker='o', color='blue', label='Mouse 33 '+bevavior+' DFF')\n",
    "        title = whichIndicator+'At' + bevavior + '_DD'\n",
    "\n",
    "\n",
    "    xDays = (SummaryD1['Days']-1).tolist()\n",
    "    ax.set_xticks(xDays)\n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    plt.legend(loc=1)\n",
    "    plt.savefig(title)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cccf8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ER_DF(0, 'rs', 'ER', compareOrNot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75da9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0b123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97319cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac370b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1331f2c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace(groundtruth=False):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(b+c, lw=2, label='denoised')\n",
    "    \n",
    "    if groundtruth:\n",
    "        plt.plot(true_b+true_c, c='r', label='truth', zorder=-11)\n",
    "    plt.plot(y, label='data', zorder=-12, c='y')\n",
    "    plt.legend(ncol=3, frameon=False, loc=(.02,.85))\n",
    "    simpleaxis(plt.gca())\n",
    "    \n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(s, lw=2, label='deconvolved', c='g')\n",
    "    \n",
    "    if groundtruth:\n",
    "        for k in np.where(true_s)[0]:\n",
    "            plt.plot([k,k],[-.1,1], c='r', zorder=-11, clip_on=False)\n",
    "    plt.ylim(0,1.3)\n",
    "    plt.legend(ncol=3, frameon=False, loc=(.02,.85));\n",
    "    simpleaxis(plt.gca())\n",
    "    \n",
    "    if groundtruth:\n",
    "        print(\"Correlation of deconvolved activity  with ground truth ('spikes') : %.4f\" % np.corrcoef(s,true_s)[0,1])\n",
    "        print(\"Correlation of denoised fluorescence with ground truth ('calcium'): %.4f\" % np.corrcoef(c,true_c)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we generate some simulated fluorescence data and plot it\n",
    "true_b = 2\n",
    "y, true_c, true_s = map(np.squeeze, gen_data(N=1, b=true_b, seed=0))\n",
    "c, s, b, g, lam = deconvolve(y, penalty=1)\n",
    "\n",
    "\n",
    "plot_trace(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f949bc8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7769c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps=15\n",
    "df = as1\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff308f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Frames, n_neuron = df.shape\n",
    "\n",
    "if Frames != 0 and n_neuron != 0:\n",
    "    # get array\n",
    "    df_array = df.to_numpy()\n",
    "\n",
    "    spike_num = 0\n",
    "    spike_mean = []\n",
    "    spikeDFF_mean = []\n",
    "\n",
    "    for i in range(n_neuron):\n",
    "        y = df_array[:,i][~np.isnan(df_array[:,i])]\n",
    "        c, s, b, g, lam = deconvolve(y, penalty=1)\n",
    "\n",
    "        # pick non-zero spikes from deconvoluated trace\n",
    "        sI = [i for i, e in enumerate(s) if e != 0]\n",
    "        ss = [e for i, e in enumerate(s) if e != 0]\n",
    "\n",
    "        # eliminate small spikes (only keep ones larger than non-zero mean)\n",
    "        meanS = np.mean(ss)\n",
    "        spikeIndex = [i for i, e in enumerate(ss) if e >= meanS]\n",
    "        spikes = [e for i, e in enumerate(ss) if e >= meanS]\n",
    "        spike_num = spike_num + len(spikeIndex)\n",
    "        spike_mean.append(np.mean(spikes))\n",
    "        spikeDFF_mean.append(np.mean(df_array[:,i][spikeIndex]))\n",
    "        \n",
    "\n",
    "    ER = spike_num/(Frames/fps)/n_neuron\n",
    "    AM_spike = np.nanmean(spike_mean)\n",
    "    AM_DFF = np.nanmean(spikeDFF_mean)\n",
    "else:\n",
    "    ER = np.nan\n",
    "    AM_spike = np.nan\n",
    "    AM_DFF = np.nan\n",
    "    \n",
    "print(ER, AM_spike, AM_DFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ER, AM_spike, AM_DFF = ER_AM_Oasis(df, fps)\n",
    "print(ER, AM_spike, AM_DFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "y = df_array[:,i][~np.isnan(df_array[:,i])]\n",
    "c, s, b, g, lam = deconvolve(y, penalty=1)\n",
    "plot_trace(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e27d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b2d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed1708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
